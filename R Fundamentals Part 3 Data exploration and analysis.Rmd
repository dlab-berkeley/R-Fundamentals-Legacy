---
title: 'R Fundamentals Part 3: Data exploration and analysis'
author: "D-Lab"
date: "September 8, 2018"
output:
  html_document:
    toc: yes
    toc_float: yes
  word_document:
    toc: yes
---
 
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Part 2 review
1. Set your working directory to the R-Fundamentals folder
```{r}
getwd()
```

1. Load data from files
```{r}
# load the animals dataframe 
animals <- read.csv("animals.csv", header = TRUE, stringsAsFactors = F)
head(animals)
str(animals)
```

2. One-dimensional subsetting
3. Two-dimensional subsetting: column names, positive integers, negative integers, logical indices  

4. Manually defining factor variables  

Coerce animals `Type` to factor type. See (this page)[https://www.stat.berkeley.edu/classes/s133/factors.html] for help with factors. 
```{r}
str(animals)
animals$Type <- factor(animals$Type, 
                       levels = c("Dog", "Pig", "Cat"),
                       ordered = T)
animals$Type
str(animals)
```

Part 3 Learning objectives  
1. Data exploration - summarization and plotting  
2. What is hypothesis testing?  
3. Fast intro to statistical testing  
4. ggplot2

# Data exploration and analysis in R
Research design, data collection, exploration, visualization, and analysis are fundamental aspects of research. Learning how to explore and analyze data will help you "think with the data" so that you become able to formulate better research designs and more efficient data collection protocols for your own research.  

Furthermore, when you read peer-reviewed work in your field, having an understanding of basic summaries, plots, and statistical tests will help you better grasp arguments that the authors are making and will allow you to more critically evaluate their rationale.  

Think about this as a research pathway of some sort. It is common to have some _a priori_ defined research questions, hypotheses, and expectations of the data. You have learned a little R and have identified a problem to solve. It's always good to first define your purpose, goals, and aims clearly. Then, you probably want to:  

1) subset/clean up your data  
2) compute descriptive statistics  
3) make some plots  
4) run statistical tests  

Descriptive statistics and plots help us check for any obvious (or not so obvious) signals or relationships. Statistical testing can then be employed to test such relationships.  

> DISCLAIMER: Remember that we are not formally teaching you statistics, but instead the basics of responsible statistical programming application in R.  

# Data summarization
This section will cover a few ways to quickly produce descriptive statistics.  

### Data summarization (`summary`)
It is often useful to first summarize your data. In R, this can be done in a variety of ways. 

`summary` provides a six-number summary of a data frame:
```{r}
summary(animals)

# or of a single vector
summary(animals$Height)
```

### Data summarization (`describe` and `describeBy`)
`describe` and `describeBy` from the `psych` R package provide some other metrics. We are going to subset `animals` so that it only includes the numeric variables within the `describe` call. 

Remember, we installed the 'psych' package on Day 1, so all we have to do is call it into our environment with `library`:
```{r}
install.packages("psych")
library(psych)

describe(animals) # this works, but causes errors in the non-numeric/integer data

str(animals)
describe(animals[ , 3:5]) # this looks cleaner after we removed columns 1 and 2 ("Type" and "Healthy")
```

We can also subset these outputs to include only the measurements we want. First, save our describe object in a variable: 
```{r}
animals_describe <- describe(animals[ , c(3:5)])
animals_describe
```

Now we can subset it like we would a regular data frame.  

If we just want "mean", "median", and "sd", we could subset our data frame just like you learned in Part 2:
```{r}
animals_simple <- animals_describe[ , c("mean", "median", "sd")]
animals_simple
```

This is convenient to quickly write output to file and copy/paste into your paper or presentation: 
```{r, eval=FALSE}
write.csv(animals_simple, "animals_simple.csv", row.names = TRUE)
#`row.names=TRUE` ensures that row names "Weight", "Height", and "Progress" labels are printed.
```

Check your working directory for the new .CSV file! 
```{r}
dir()
```

We can also describe our data by a grouping variable using `describeBy()`. What if we want summary statistics not for the whole sample like above, but for each animal type (i.e., Cat, Dog, and Pig?)
```{r, eval=FALSE}
#Output summary statistics by one grouping variable:
summary_sub <- describeBy(animals[ , c(3:5)], group = animals$Type)
summary_sub

#If we just want to view Cats, Dogs, or Pigs, we can type:
summary_sub$Cat
summary_sub$Dog
summary_sub$Pig

#We can also view just the means for Dogs, we can type:
summary_sub$Dog[["mean"]] # or
summary_sub$Dog[[3]]

#If we just want the second value (Height mean) for Dogs (8.5), we can type:
summary_sub$Dog[["mean"]][2]

#Or the medians for Pigs:
summary_sub$Pig[[5]]
summary_sub$Pig[["median"]]
summary_sub$Pig[["median"]][1] # just the first value
```

### Data summarization (`table`)
We can view frequencies for of categorical data like `animals$Type` with `table`
```{r}
table(animals$Type) # get frequencies for Cat, Dog, and Pig

table(animals$Type, animals$Healthy) #get frequencies for Cat, Dog, and Pig by column "Healthy".
```

Wow! Only 2/7 cats are healthy.  
3/6 dogs are healthy.  
6/7 pigs are healthy.   

Thus, seemingly simple tabulations can help us make informed decisions. Why are cats the least healthy? Perhaps now we can devote resources to the care of cats.  

# Plotting
Visualizing data is useful for exploration. R offers a variety of base plots and a multitude of additional packages for plotting. Start with histograms and boxplots in base R. Scatterplots are only topically covered here because the `ggplot2` package produces them much more efficiently.  

### Histogram (`hist`)  
Histograms are a handy way to illustrate the distribution shape of one numeric/integer variable. Let's visualize the "Height" variable from the `animals` data frame:
```{r}
hist(animals$Weight, 
     col = "gray90")
```

The `col = ` argument specifies the color of the bars. Look at the colors you can choose from in base R using `colors()`
```{r, eval=FALSE}
colors() 
```

Some basic arguments you will want to make note of:  
-`main` - change plot title  
-`xlab` - change x-axis label  
-`ylab` - change y-axis label  
-`las` - change orientation of the tick mark text  
```{r}
hist(animals$Weight, 
     col = "gray80",
     main = "Histogram of animal weight",
     xlab = "Animal weight (kg)",
     ylab = "FREQUENCY",
     las = 1)
```

Click the "Zoom" button to pop-out the plot into its own window. Click "Export" to save it as an image or .pdf file!  

# **Challenge 1**
Use `summary()`, `table()` and `describeBy()` to investigate variables in the `iris` dataset. Create a histogram of one of the numeric variables.  
```{r}
data(iris)
summary(iris)
table(iris$Species)
describeBy(iris, group = iris$Species)
```

### Boxplots (`boxplot`)  
Boxplots also are useful because you can graphically represent a numeric variable by levels of a factor - this requires slightly different syntax. These default to [Tukey boxplots](http://mathworld.wolfram.com/Box-and-WhiskerPlot.html).  
```{r}
boxplot(animals$Height ~ animals$Type)
```
The tilde `~` means "as parsed by" or "as divided by" some category (in our case Height as parsed _by_ Cat, Dog, and Pig). It means we want to look at animal Height by animal Type.  

Remember your six number summary from earlier? A Tukey boxplot represents a 5 number summary (minus the mean).  

The **"whiskers"** of these box and whisker plots represent the minimum and maximum values.  

The **lower and upper** borders of the rectangle represent the first and third quartiles.  

The thick **horizontal** black bar inside the box represents the median.  

We can change box colors by specifying a vector of color names with a length of 3 (one for each level of our factor). Other base plotting features apply as well:  
```{r}
boxplot(animals$Height ~ animals$Type,
        col = c("aquamarine", "goldenrod", "salmon"),
        main = "Animal boxplots", 
        xlab = "Type", ylab = "Height", 
        las = 1)
#That looks better! 
```

We can also save our figures as .PDF files using code:
```{r, eval = F}
?pdf
?tiff
```
```{r, eval=FALSE}
pdf("animals boxplot.pdf", 6, 6)
boxplot(animals$Height ~ animals$Type,
        col = c("aquamarine", "goldenrod", "salmon"),
        main = "Animal boxplots", 
        xlab = "Type", ylab = "Height", 
        las=1)
dev.off()
```
Check your working directory again - you now have a .PDF of your boxplots! 

# **Challenge 2**
Create boxplots for one of the numeric variables in the `iris` dataset and export it to your working directory as a .TIFF file. 
```{r}
data(iris)
boxplot(iris$Sepal.Length ~ iris$Species,
        col = c("red", "blue", "green"),
        main = "Iris (Sepal Length ~ Species) boxplots",
        xlab = "Type",
        ylab = "Sepal Length (cm)",
        las = 1)
```


### Scatterplots (`plot()`)  
What if we want to know more about the relationships between two numeric or integer variables? Base R's `plot()` can be useful for quick scatterplots. 

Remember earlier that we coerced "Type" from character to factor data type? This helps us map colors and shapes to specific factor levels: 
```{r}
str(animals)
```

Then, identify two numeric/integer variables you want to plot on the x and y axes:
```{r, eval=FALSE}
animals$Weight
animals$Height
```
Now, we can plot animal "Weight" versus "Height":
```{r}
plot(x = animals$Weight, y = animals$Height,    
    xlab = "Weight (kg)",   # change x axis label
    ylab = "Height (cm)",   # change y axis label
    main = "Animal weights and heights",   # add plot title
    las = 1, # make all axis text parallel to x-axis
    col = as.integer(animals$Type),   # change point colors to correspond to animal types
    pch = as.integer(animals$Type),   # change point symbols to correspond to animal types
    cex = 2)   # change point size 
```

There is extensive help available for finding help with colors and point shapes in R. See [here](https://www.stat.ubc.ca/~jenny/STAT545A/block14_colors.html), [here](https://www.stat.ubc.ca/~jenny/STAT545A/block15_colorMappingBase.html), and [here](http://www.sthda.com/english/wiki/r-plot-pch-symbols-the-different-point-shapes-available-in-r).  

However, in our examples the points are falling ourside the plot boundaries! We can change that with `xlim` and `ylim`:
```{r}
plot(animals$Weight, animals$Height,    
    xlab = "Weight (kg)",   # change x axis label
    ylab = "Height (cm)",   # change y axis label
    main = "Animal weights and heights",   # add plot title
    las = 1, # make all axis text parallel to x-axis
    col = as.integer(animals$Type),   # change point colors to match animal types
    pch = as.integer(animals$Type),   # change point symbols to match animal types
    cex = 2,   # change point size 
    xlim = c(3,8),  # adjust range of x axis
    ylim = c(4,11))  # adjust range of y axis

# That looks better! We can also add a legend:

legend("topright", inset = 0, title = "Animal", cex = 1,
       c("Cat","Dog","Pig"), col = c(1,2,3), pch = c(1,2,3),
       horiz = FALSE)
```

> NOTE: One drawback of R Studio is that its in-environment graphics look a little strange sometimes. However, to save it as a beautiful .PNG file, we wrap our plotting and legend instructions by `png()` and `dev.off()` lines of code. We can specify figure characteristics inside `png()` such as the dimensions and resolution of the image, while `dev.off()` writes the file to disk.

> Alternatively, you can simply click the "export" button in your plot GUI window pane.  

Highlight and run all of this code at once. This will create a 6 x 6 inch figure at 300 dpi.  
```{r, eval=FALSE}
png("Animal weights and heights.png", height=6, width=6, units="in", res=300)
plot(animals$Weight, animals$Height,    
    xlab = "Weight (kg)", ylab="Height (cm)", 
    main = "Animal weights and heights", las=1,
    col = as.integer(animals$Type), pch = as.integer(animals$Type), 
    cex = 2, xlim = c(3,8), ylim = c(3,11))
legend("topright", inset = .0, title = "Animal", cex = 1,
       c("Cat","Dog","Pig"), col = c(1,2,3), pch = c(1,2,3),
       horiz = FALSE)
dev.off()
```
VoilÃ ! Check your working directory - we now have a publishable quality figure that can be copy and pasted into your manuscript.   

# **Challenge 3**
Using the `iris` dataset, plot two numeric variables and export the graph to your working directory as a .PNG file. 

# ggplot2
The "ggplot2" R package is a powerful way to graph your data. The syntax is similar to what is used in "dplyr", except instead of a pipe `%>%`, a plus-symbol `+` is used.  

The [ggplot2](http://ggplot2.org/) R package eases the burden of plotting using base functions. Instead, it favors simplified syntax, which is different from anything you have seen thus far. ggplot2 is part of the [tidyverse](https://www.tidyverse.org/) - a series of R packages designed to simplify R syntax. Click the above hyperlink to learn more. "gg" stands for "Grammar of Graphics." 

The `ggplot` function is the main function call. You need three things to create a ggplot:  
1) a dataset - some data to be visualized  
2) "aes"thetics - definitions of your coordinate system, point color and shape mappings, etc.  
3) geoms - how your data should be represented: points, bars, densities, ribbons, etc.

ggplot2 functions work in layers. Each time you wish to add a geom, title, axis label, or background color, you must add a new layer by typing the plus `+` sign.  

Check out the help files and then see the below examples using the "iris" dataset:
```{r}
install.packages("ggplot2")
library(ggplot2)
?ggplot2
data(iris)
```

# ggplot histogram 
Remember how we said the `ggplot` function works in layers? Look what happens if we specifiy our data set ("iris") and our global aesthetic ("aes"), but we do not specify our "geom". We get the coordinate system and a defined X axis (Petal.Length), but no representation of the data!
```{r}
ggplot(data = iris, aes(x = Petal.Length))
```

We can add the bars by adding a new layer with `+`, and then specify "geom_histogram()" to get produce the histogram! 

To see the list of available geoms, type "geoms_" and press the `TAB` key. 
```{r}
ggplot(data = iris, aes(x = Petal.Length)) + 
    geom_histogram()
```

You can also change the color of the bars by adding the `fill` argument to the global aes, change the background color by adding `theme_` in a new layer, and the title by adding `ggtitle` in a new layer. The `bins` argument will change the number and width of the histogram bars.
```{r}
ggplot(data = iris, aes(x = Petal.Length)) + 
    geom_histogram(fill = "blue", color = "black", bins = 10) + 
    theme_classic() + 
    ggtitle("This is a ggplot histogram") + 
  ylab("Ice Cream")
```

# ggplot boxplots
Boxplots continue to follow this logic. However, this time we should specifically define our X axis as the factor variable we wish to investigate, and the Y axis as some numeric value: 
```{r}
ggplot(data = iris, aes(x = Species, y = Petal.Length, fill = Species)) + 
    geom_boxplot() + 
    theme_minimal() + 
    ylab("Petal Length (cm)") +
    ggtitle("This are ggplot boxplots") # + 
    # guides(fill = FALSE) 
```
What happens if you un-hashtage `guides(fill = FALSE)` in the above chunk?  

# ggplot scatterplots
Scatterplots work similarly. However, this time we must define both our X and Y axes in our global "aes" and select "geom_point()" as our geom. 

We can then automatically map the factor variable ("Species") to point types and shapes in the `color` and `shape` arguments. By doing so, a legend is automatically created for us! 

The other layer arguments apply as well. `size` will change the size of the points, while `alpha` will change their transparency. 

We can also include a second `theme` argument layer where can can specify the legend position via `legend.position`. 

The X and Y axis labels can be changed via `xlab` and `ylab` layers, for example if we want to include our units of measurement (centimeters):
```{r}
ggplot(data = iris, aes(x = Petal.Length, y = Petal.Width, color = Species, shape = Species)) + 
    geom_point(size = 10, alpha = 0.5) + 
    theme_classic() +
    ggtitle("This is a ggplot scatterplot") +
    theme(legend.position = "top") + 
    xlab("Petal Length (cm)") + 
    ylab("Petal Width (cm)") + 
    theme(legend.position = "top",
          plot.title = element_text(hjust = 0.5, size = 40),
          legend.title = element_text(size = 20),
          legend.text = element_text(size = 20),
          axis.text.x = element_text(angle = 45, hjust = 1)) 
```

# **Challenge 4** 
In lines the ggplot code above, what are the arguments inside of our second "theme" argument doing?

# Statistical testing
Now that we have sufficiently summarized and explored our data, it is time to formally investigate the basics of a few statistical methods. Remember that you must investigate the assumptions of all of these tests, which are not covered here.  

### Hypothesis testing
You will frequently encounter hypotheses in the peer-reviewed literature and in the output of test results in R. Hypotheses generally should be based on research questions and expectations about the data. 

**Hypothesis testing** is central to research. How can you tell if the differences you observe are statistically real or not? We answer the question through hypothesis formulation and significance testing as measured by p-values.  

The simplest way to think about hypothesis testing is that you have two hypotheses: the null (Ho) and the alternative (Ha). 

The null hypothesis can be thought of as **NO DIFFERENCE / RELATIONSHIP / CORRELATION**  

The alternative hypothesis can be thought of as **SOME SORT OF DIFFERENCE / RELATIONSHIP / CORRELATION**  

### p-value
In this oversimplified example these are your only two options and are tested using a "p-value". A p-value signifies how likely differences are statistically real instead of due to random chance. Or, it specifies the probability of finding a value as or more extreme under the null hypothesis. The most common standard cutoff value is p<0.05.  

If p > 0.05, you **FAIL TO REJECT THE NULL HYPOTHESIS**! This means that the null hypothesis holds true and that there is "no difference / relationship / correlation" among the variables being tested. By default we accept the conditions of the null hypothesis.  

If p < 0.05, you **REJECT THE NULL HYPOTHESIS** of "no difference / relationship / correlation" and by default you accept the alternative hypothesis and whatever "difference / relationship / correlation" it specified. This is a **_"statistically significant"_** difference.  

Even though they _look_ similar, we can ask: are there formal statistically significant differences between the means of versicolor and virginica petal lengths?

Well, we can test those assumptions with "mean comparison" tests such as t-tests and analysis of variance (ANOVA)!  

### Statistical testing (`t.test`)  
A "t.test" formally compares **one or two group means** for statistically significant differences at an often standard p-value cutoff of p<0.05. We can use `t.test` to make an observation of a population based on a sample. We are only examining a handful of petal lengths - if we had ALL petal lengths there would be no need for a statistic - instead, we would only be concerned with what we observe!   

> KEY POINT: a t-test should only be applied for comparisons between one or two groups. Use an ANOVA (discussed below) for _more than two groups!_  

The null hypothesis states that there are no actual mean differences between the two groups. 

# **Challenge 5**
Subsetting review! Subset your "animals" data frame into three new dataframes: setosa, versicolor, and virginica that each contain only their respective Species:
```{r, inclue = F}
## YOUR CODE HERE
```

Now, we can compare petal lengths between two of the three iris species (we will use an ANOVA to compare all three group means while correcting for family wise error - see below). 
```{r}
t.test(versicolor$Petal.Length, virginica$Petal.Length)
# The p-value is 2.2e-16 (very small). Reject the null hypothesis and accept the conditions of the alternate - that there is indeed some statistically significant difference between petal lengths of versicolor and virginica species. 
```

# **Challenge 6**
Create two numeric vectors. Perform a t.test on them:
```{r}
## YOUR CODE HERE
```

### Statistical testing (`aov` and `TukeyHSD`)
What about if we want to compare more than two groups? Doing multiple pairwise t-tests for **1)** setosa v. versicolor, **2)** setosa v. virginica, and **3)** versicolor v. virginica is not ideal because it becomes more difficult to adjust for the [family wise error rate](https://en.wikipedia.org/wiki/Family-wise_error_rate), or the influence of the other groups/variables that are present but not necessarily being being tested.

When we want to compare _more than two group means at once_ (i.e., setosa, versicolor, _and_ virginica), we can use an analysis of variance (ANOVA). This is called by `aov` in R.

We can follow `aov` with a "Tukey test of Honest Significant Difference" to find out between exactly which groups the differences (if any) exist. This is called with `TukeyHSD` in R.

Let's try! Note the slightly different syntax - we can now specify only the column headings, and enter the name of our data frame in the `data` argument. Let's call our object `aov1`, which is an object we can unpack and look inside:
```{r}
aov1 <- aov(Petal.Length ~ Species, data = iris)

#Use `summary()` to access the useful information from our `aov1` model:
summary(aov1)
```

We receive a highly significant p-value. But how do we know between which groups the differences exist? We can follow this up with a TukeyHSD test to see which between-group differences contribute most:
```{r}
TukeyHSD(aov1)
```

This allows us to see mean differences bewteen the multiple species, with adjusted p-values. All are "0". However, we are provided with the mean difference ("diff") and the lower ("lwr") and upper ("upr") confidence interval boundaries.  

# **Challenge 7**
Using the `mtcars` dataset, perform an ANOVA and TukeyHSD test for one of the numeric variables between the three cyl sizes. In general, what are differences between t-test and ANOVA that you should keep in mind?  
```{r}
## YOUR CODE HERE
```

### Statistical testing (`cor.test`)  
Linear [correlation](https://en.wikiversity.org/wiki/Correlation) is a useful way to see if two numeric variables are related. "Pearson's r" is the default coefficient in `cor.test()`. Correlation is a number that ranges between -1 and 1 with -1 indicating a negative correlation and 1 indicating a positive correlation. 

Investigate if Petal.Length and Sepal.Length are correlated:
```{r}
cor.test(iris$Petal.Length, iris$Sepal.Length)
```

Definitely! Results indicate a strong positive correlation (cor = 0.8717538) that is also highly significant (p < 2.2e-16).  

> NOTE! Remember that correlation does not equal causation! 

### Statistical testing (`lm`)  
Now what? Linear regression is a convenient way to see if one numeric variable can be used to predict another. Do you think Sepal.Width can be used to predict Petal.Length?  

Again, note the altered syntax: `model1 <- lm(Y ~ X, data=data)`

Y is the **DEPENDENT**, target, response, or outcome variable. This is the variable we want to predict.

X is the **INDEPENDENT**, predictor, input, or explanatory variable. 

We can use X (Sepal.Width) to predict the outcome of Y (Petal.Length) using the iris data set!
```{r}
lin_model1 <- lm(Petal.Length ~ Sepal.Width, data = iris)
summary(lin_model1)
```

`lm()` output is dense! Check out [the yhat blog for fitting and interpreting linear models](http://blog.yhat.com/posts/r-lm-summary.html). Also be aware that we have not discussed [data assumptions](http://www.statisticssolutions.com/assumptions-of-linear-regression/) for using `lm` or any of the other tests in this file, or when it is specifically appropriate (or inappropriate) to use them. 

> DISCLAIMER! If you are interested in any of these tests presented in this lesson, make sure you consult the prerequisite data assumptions for these tests (i.e., normality, trait independence, etc.), with a statistician.

You can also pull items out of return objects using `names()`. To extract the residuals we would use the dollar sign operator `$` and type:

```{r, eval=FALSE}
names(lin_model1)
hist(lin_model1$residuals, col = "lightgreen")
```

# **Challenge 8**
1. Load the `mtcars` dataset.  
2. When you load new data into R, what functions are good ones to run to learn more about the data?  
3. Create boxplots for mpg as parsed by cyl. 
4. Might you surmise a relationship about something like engine size and miles per gallon?  
5. What does `cor.test` reveal about mpg and cyl?  
6. Create a scatterplot of the "mpg" and "hp" variables using `ggplot()`. What happens?  
7. Can cyl be used to predict mpg in a linear regression model?  

> REMEMBER: If you have research questions, [schedule an appointment with a D-Lab consultant!](http://dlab.berkeley.edu/consulting)

Acknowledgements:
- Contributions by Evan Muzzall, Rochelle Terman, Dillon Niederhut. 